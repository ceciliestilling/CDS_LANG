{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e68f705d-2341-456b-8359-de78248ac417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:20:31.218708Z",
     "iopub.status.busy": "2022-03-01T16:20:31.217358Z",
     "iopub.status.idle": "2022-03-01T16:20:31.230038Z",
     "shell.execute_reply": "2022-03-01T16:20:31.228709Z",
     "shell.execute_reply.started": "2022-03-01T16:20:31.218561Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import math\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa06d38-ecb8-40f6-bc48-aea45dd65db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:10:58.615237Z",
     "iopub.status.busy": "2022-03-01T16:10:58.614330Z",
     "iopub.status.idle": "2022-03-01T16:10:58.627291Z",
     "shell.execute_reply": "2022-03-01T16:10:58.625983Z",
     "shell.execute_reply.started": "2022-03-01T16:10:58.615122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare tokenization\n",
    "def tokenization(input_str):\n",
    "    # Split on any non-alphanumeric character\n",
    "    tokenizer = re.compile(r\"\\W+\")\n",
    "    \n",
    "    # Tokenize \n",
    "    token_list = tokenizer.split(input_str)\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "589e1820-592c-4f34-8858-dba5a09c8dd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:11:03.281955Z",
     "iopub.status.busy": "2022-03-01T16:11:03.281060Z",
     "iopub.status.idle": "2022-03-01T16:11:03.294919Z",
     "shell.execute_reply": "2022-03-01T16:11:03.293335Z",
     "shell.execute_reply.started": "2022-03-01T16:11:03.281838Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep: Number of times search term is present with given collocates\n",
    "# st means search term\n",
    "def calc_st_w_coll(concordances):\n",
    "\n",
    "    st_w_coll = {} \n",
    "\n",
    "    for conc in concordances:\n",
    "        for collocate in conc:\n",
    "            if collocate in st_w_coll:\n",
    "                st_w_coll[collocate] += 1\n",
    "            else:\n",
    "                st_w_coll[collocate] = 1\n",
    "\n",
    "    return st_w_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62590202-2e50-4467-97c1-ad32bf80238f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:11:04.076485Z",
     "iopub.status.busy": "2022-03-01T16:11:04.075578Z",
     "iopub.status.idle": "2022-03-01T16:11:04.091905Z",
     "shell.execute_reply": "2022-03-01T16:11:04.090936Z",
     "shell.execute_reply.started": "2022-03-01T16:11:04.076368Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep: Calculate collocates w/o search term and total of collocates\n",
    "def calc_coll_wo_st_total_coll(concordances, all_tokens, st_w_coll_dict):\n",
    "    coll_wo_st = {}\n",
    "    total_coll = {}\n",
    "\n",
    "    for conc in concordances:\n",
    "        for collocate in conc:\n",
    "            total_count = all_tokens.count(collocate)\n",
    "            total_coll[collocate] = total_count\n",
    "            wo_st = total_count - st_w_coll_dict[collocate]\n",
    "            coll_wo_st[collocate] = wo_st\n",
    "    \n",
    "    return (coll_wo_st, total_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1475cd52-84a4-4e71-9c6f-8e662e1c03fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:11:05.076567Z",
     "iopub.status.busy": "2022-03-01T16:11:05.075687Z",
     "iopub.status.idle": "2022-03-01T16:11:05.091693Z",
     "shell.execute_reply": "2022-03-01T16:11:05.090298Z",
     "shell.execute_reply.started": "2022-03-01T16:11:05.076450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep: Number of times search term is present without any given collocate\n",
    "def calc_stcount_st_wo_coll(all_tokens, st, st_w_coll_dict):\n",
    "    st_wo_coll = {}\n",
    "\n",
    "    # Number of times the search term occurs across the corpus\n",
    "    st_count = all_tokens.count(st)\n",
    "    for collocate, count in st_w_coll_dict.items():\n",
    "        st_wo_coll[collocate] = st_count - count\n",
    "\n",
    "    return (st_count, st_wo_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60e7fd83-89c0-4a46-bdcd-7a97e4597d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:24:20.544490Z",
     "iopub.status.busy": "2022-03-01T16:24:20.543445Z",
     "iopub.status.idle": "2022-03-01T16:24:20.556215Z",
     "shell.execute_reply": "2022-03-01T16:24:20.554783Z",
     "shell.execute_reply.started": "2022-03-01T16:24:20.544369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep: The expected frequency of a given collocate occurring with the search term\n",
    "def calc_exp_freq(total_coll, stcount, corpus_size):\n",
    "    \n",
    "    exp_freq = {}\n",
    "\n",
    "    for collocate in total_coll: \n",
    "        exp_freq[collocate] = (stcount * total_coll[collocate]) / N\n",
    "\n",
    "    return exp_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1dfee48-2b85-4a79-856a-476e1ddb65fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:11:06.490517Z",
     "iopub.status.busy": "2022-03-01T16:11:06.489682Z",
     "iopub.status.idle": "2022-03-01T16:11:06.508307Z",
     "shell.execute_reply": "2022-03-01T16:11:06.507104Z",
     "shell.execute_reply.started": "2022-03-01T16:11:06.490403Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep: The mutual information for any given collocate to the search term\n",
    "def calc_mut_inf(st_w_coll, exp_freq):\n",
    "    \n",
    "    mut_inf = {}\n",
    "\n",
    "    for collocate in st_w_coll:\n",
    "        mut_inf[collocate] = math.log(st_w_coll[collocate] / exp_freq[collocate])\n",
    "\n",
    "    return mut_inf\n",
    "\n",
    " # Write headers to csv - separately from appending, since this also overwrites the old file\n",
    "def write_csv(outfile, st_w_coll, mut_inf):\n",
    "   \n",
    "    with open(outfile, 'w', encoding='utf-8') as fh:\n",
    "        fh.write('collocate,raw_frequency,MI\\n')\n",
    "\n",
    "    with open(outfile, 'a', encoding='utf-8') as fh:\n",
    "        for collocate in O11:\n",
    "            fh.write(f'{collocate},{st_w_coll[collocate]},{mut_inf[collocate]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "530f72e6-4014-4800-9157-9d34273a8b1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:19:13.870833Z",
     "iopub.status.busy": "2022-03-01T16:19:13.869914Z",
     "iopub.status.idle": "2022-03-01T16:19:13.894494Z",
     "shell.execute_reply": "2022-03-01T16:19:13.893688Z",
     "shell.execute_reply.started": "2022-03-01T16:19:13.870712Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(data_dir, st, window_size, sample_num):\n",
    "    # Output file is named after the st and window size\n",
    "    outpath = 'output'\n",
    "    outfile = os.path.join(outpath, f'{st}_{window_size}.csv')\n",
    "\n",
    "    # Make everything lowercase \n",
    "    st = st.lower()\n",
    "\n",
    "    # The list of all tokens in the corpus\n",
    "    tokens = [] \n",
    "\n",
    "    for i, novel_path in enumerate(data_dir.glob('*.txt')): # enumerate() method adds a counter to an iterable and returns it in a form of enumerating object\n",
    "        # Only use a subset of files if running as demo\n",
    "        if i == sample_num: \n",
    "            break\n",
    "\n",
    "        with open(novel_path, 'r', encoding='utf-8') as fh:\n",
    "            content = fh.read()\n",
    "            # Splits the whole novel-content string into tokens on non-word characters\n",
    "            tokens += tokenization(content)\n",
    "\n",
    "    # Clowercase tokens yes please\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Total number of tokens in corpus\n",
    "    corpus_size = len(tokens)\n",
    "\n",
    "    # Locations of search term in the token list\n",
    "    st_indices = [i for i, token in enumerate(tokens) if token == st.lower()]\n",
    "\n",
    "    # A list of tokens slices +- window_size around search term\n",
    "    concordances = [tokens[max(0, i - window_size) : i + window_size + 1] for i in st_indices]\n",
    "\n",
    "    # Filters out the exact search term that we are checking against, but not other occurences of the search term\n",
    "    for conc in concordances:\n",
    "        # Removes the window_size'th element (exact middle) of concordance line, which is the search term\n",
    "        conc.pop(window_size)\n",
    "\n",
    "    # Number of times search term is present with given collocates\n",
    "    st_w_col = calc_st_w_col(concordances)\n",
    "\n",
    "    # coll_wo_st: Number of times given collocate occurs without search term\n",
    "    # total_coll: Total number of times a given collocate occurs\n",
    "    coll_wo_st, total_coll = calc_coll_wo_st_total_coll(concordances, tokens, st_w_coll)\n",
    "    \n",
    "    # R1 / stcount: Number of times the search term occurs across the corpus\n",
    "    # st_wo_coll: Number of times search term is present without any given collocate\n",
    "    R1, st_wo_coll = calc_stcount_st_wo_coll(tokens, st, st_w_coll)\n",
    "\n",
    "    # The expected frequency of a given collocate occurring with the search term\n",
    "    exp_freq = calc_exp_freq(total_coll, R1, corpus_size)\n",
    "\n",
    "    # The MI for any given collocate to the search term\n",
    "    mut_inf = calc_mut_inf(st_w_col, exp_freq)\n",
    "\n",
    "    # Write 3 columns: collocate, raw_frequency, MI\n",
    "    write_csv(outfile, st_w_coll, mut_inf)\n",
    "    \n",
    "    print('Data written to: ' + outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6deeb24c-7e42-409b-b05f-77083d44afcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T16:20:35.338827Z",
     "iopub.status.busy": "2022-03-01T16:20:35.337901Z",
     "iopub.status.idle": "2022-03-01T16:20:35.371519Z",
     "shell.execute_reply": "2022-03-01T16:20:35.370510Z",
     "shell.execute_reply.started": "2022-03-01T16:20:35.338706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_192/1873448380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--window_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the number of words on both sides of the keyword to look for collocates in'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--sample_num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'whether to only use a subset of files and how many to use'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--data_dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the directory containing all of your text files to analyze'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='calculate collocates for a specific keyword')\n",
    "    parser.add_argument('keyword', help='the keyword to look for')\n",
    "    parser.add_argument('-w', '--window_size', type=int, default=5, help='the number of words on both sides of the keyword to look for collocates in')\n",
    "    parser.add_argument('-s', '--sample_num', type=int, help='whether to only use a subset of files and how many to use')\n",
    "    parser.add_argument('-d', '--data_dir', type=Path, default = Path('./data/'), help='the directory containing all of your text files to analyze')\n",
    "    args = parser.parse_args()\t\n",
    "\n",
    "    main(keyword = args.keyword, window_size = args.window_size, sample_num = args.sample_num, data_dir = args.data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
